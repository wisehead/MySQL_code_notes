#<center>lock</center>

#1.locktree

```cpp
    // A locktree represents the set of row locks owned by all transactions
    // over an open dictionary. Read and write ranges are represented as
    // a left and right key which are compared with the given comparator
    //
    // Locktrees are not created and destroyed by the user. Instead, they are
    // referenced and released using the locktree manager.
    //
    // A sample workflow looks like this:
    // - Create a manager.
    // - Get a locktree by dictionaroy id from the manager.
    // - Perform read/write lock acquision on the locktree, add references to
    //   the locktree using the manager, release locks, release references, etc.
    // - ...
    // - Release the final reference to the locktree. It will be destroyed.
    // - Destroy the manager.
    class locktree {
    public:
        // requires: the reference count is > 0
        // returns: the reference count, after decrementing it by one
        uint32_t release_reference(void);

        // returns: the current reference count
        uint32_t get_reference_count(void);

    private:
        locktree_manager *m_mgr;
        DICTIONARY_ID m_dict_id;
        uint32_t m_reference_count;

        // Since the memory referenced by this comparator is not owned by the
        // locktree, the user must guarantee it will outlive the locktree.
        //
        // The ydb API accomplishes this by opening an ft_handle in the on_create
        // callback, which will keep the underlying FT (and its descriptor) in memory
        // for as long as the handle is open. The ft_handle is stored opaquely in the
        // userdata pointer below. see locktree_manager::get_lt w/ on_create_extra
        comparator m_cmp;

        concurrent_tree *m_rangetree;

        void *m_userdata;
        struct lt_lock_request_info m_lock_request_info;
        // The following fields and members prefixed with "sto_" are for
        // the single txnid optimization, intended to speed up the case
        // when only one transaction is using the locktree. If we know
        // the locktree has only one transaction, then acquiring locks
        // takes O(1) work and releasing all locks takes O(1) work.
        //
        // How do we know that the locktree only has a single txnid?
        // What do we do if it does?
        //
        // When a txn with txnid T requests a lock:
        // - If the tree is empty, the optimization is possible. Set the single
        // txnid to T, and insert the lock range into the buffer.
        // - If the tree is not empty, check if the single txnid is T. If so,
        // append the lock range to the buffer. Otherwise, migrate all of
        // the locks in the buffer into the rangetree on behalf of txnid T,
        // and invalid the single txnid.
        //
        // When a txn with txnid T releases its locks:
        // - If the single txnid is valid, it must be for T. Destroy the buffer.
        // - If it's not valid, release locks the normal way in the rangetree.
        //
        // To carry out the optimization we need to record a single txnid
        // and a range buffer for each locktree, each protected by the root
        // lock of the locktree's rangetree. The root lock for a rangetree
        // is grabbed by preparing a locked keyrange on the rangetree.
        TXNID m_sto_txnid;
        range_buffer m_sto_buffer;

        // The single txnid optimization speeds up the case when only one
        // transaction is using the locktree. But it has the potential to
        // hurt the case when more than one txnid exists.
        //
        // There are two things we need to do to make the optimization only
        // optimize the case we care about, and not hurt the general case.
        //
        // Bound the worst-case latency for lock migration when the
        // optimization stops working:
        // - Idea: Stop the optimization and migrate immediate if we notice
        // the single txnid has takes many locks in the range buffer.
        // - Implementation: Enforce a max size on the single txnid range buffer.
        // - Analysis: Choosing the perfect max value, M, is difficult to do
        // without some feedback from the field. Intuition tells us that M should
        // not be so small that the optimization is worthless, and it should not
        // be so big that it's unreasonable to have to wait behind a thread doing
        // the work of converting M buffer locks into rangetree locks.
        //
        // Prevent concurrent-transaction workloads from trying the optimization
        // in vain:
        // - Idea: Don't even bother trying the optimization if we think the
        // system is in a concurrent-transaction state.
        // - Implementation: Do something even simpler than detecting whether the
        // system is in a concurent-transaction state. Just keep a "score" value
        // and some threshold. If at any time the locktree is eligible for the
        // optimization, only do it if the score is at this threshold. When you
        // actually do the optimization but someone has to migrate locks in the buffer
        // (expensive), then reset the score back to zero. Each time a txn
        // releases locks, the score is incremented by 1.
        // - Analysis: If you let the threshold be "C", then at most 1 / C txns will
        // do the optimization in a concurrent-transaction system. Similarly, it
        // takes at most C txns to start using the single txnid optimzation, which
        // is good when the system transitions from multithreaded to single threaded.
        //
        // STO_BUFFER_MAX_SIZE:
        //
        // We choose the max value to be 1 million since most transactions are smaller
        // than 1 million and we can create a rangetree of 1 million elements in
        // less than a second. So we can be pretty confident that this threshold
        // enables the optimization almost always, and prevents super pathological
        // latency issues for the first lock taken by a second thread.
        //
        // STO_SCORE_THRESHOLD:
        //
        // A simple first guess at a good value for the score threshold is 100.
        // By our analysis, we'd end up doing the optimization in vain for
        // around 1% of all transactions, which seems reasonable. Further,
        // if the system goes single threaded, it ought to be pretty quick
        // for 100 transactions to go by, so we won't have to wait long before
        // we start doing the single txind optimzation again.
        static const int STO_BUFFER_MAX_SIZE = 50 * 1024;
        static const int STO_SCORE_THRESHOLD = 100;
        int m_sto_score;

        // statistics about time spent ending the STO early
        uint64_t m_sto_end_early_count;
        tokutime_t m_sto_end_early_time;

        // Effect:
        //  Provides a hook for a helgrind suppression.
        // Returns:
        //  true if m_sto_txnid is not TXNID_NONE
        bool sto_txnid_is_valid_unsafe(void) const;

        // engine status reaches into the locktree to read some stats
        friend void locktree_manager::get_status(LTM_STATUS status);
    };                                                                                
```