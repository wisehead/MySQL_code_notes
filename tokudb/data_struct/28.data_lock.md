#<center>lock</center>

#1.locktree

```cpp
    // A locktree represents the set of row locks owned by all transactions
    // over an open dictionary. Read and write ranges are represented as
    // a left and right key which are compared with the given comparator
    //
    // Locktrees are not created and destroyed by the user. Instead, they are
    // referenced and released using the locktree manager.
    //
    // A sample workflow looks like this:
    // - Create a manager.
    // - Get a locktree by dictionaroy id from the manager.
    // - Perform read/write lock acquision on the locktree, add references to
    //   the locktree using the manager, release locks, release references, etc.
    // - ...
    // - Release the final reference to the locktree. It will be destroyed.
    // - Destroy the manager.
    class locktree {
    public:
        // requires: the reference count is > 0
        // returns: the reference count, after decrementing it by one
        uint32_t release_reference(void);

        // returns: the current reference count
        uint32_t get_reference_count(void);

    private:
        locktree_manager *m_mgr;
        DICTIONARY_ID m_dict_id;
        uint32_t m_reference_count;

        // Since the memory referenced by this comparator is not owned by the
        // locktree, the user must guarantee it will outlive the locktree.
        //
        // The ydb API accomplishes this by opening an ft_handle in the on_create
        // callback, which will keep the underlying FT (and its descriptor) in memory
        // for as long as the handle is open. The ft_handle is stored opaquely in the
        // userdata pointer below. see locktree_manager::get_lt w/ on_create_extra
        comparator m_cmp;

        concurrent_tree *m_rangetree;

        void *m_userdata;
        struct lt_lock_request_info m_lock_request_info;
        // The following fields and members prefixed with "sto_" are for
        // the single txnid optimization, intended to speed up the case
        // when only one transaction is using the locktree. If we know
        // the locktree has only one transaction, then acquiring locks
        // takes O(1) work and releasing all locks takes O(1) work.
        //
        // How do we know that the locktree only has a single txnid?
        // What do we do if it does?
        //
        // When a txn with txnid T requests a lock:
        // - If the tree is empty, the optimization is possible. Set the single
        // txnid to T, and insert the lock range into the buffer.
        // - If the tree is not empty, check if the single txnid is T. If so,
        // append the lock range to the buffer. Otherwise, migrate all of
        // the locks in the buffer into the rangetree on behalf of txnid T,
        // and invalid the single txnid.
        //
        // When a txn with txnid T releases its locks:
        // - If the single txnid is valid, it must be for T. Destroy the buffer.
        // - If it's not valid, release locks the normal way in the rangetree.
        //
        // To carry out the optimization we need to record a single txnid
        // and a range buffer for each locktree, each protected by the root
        // lock of the locktree's rangetree. The root lock for a rangetree
        // is grabbed by preparing a locked keyrange on the rangetree.
        TXNID m_sto_txnid;
        range_buffer m_sto_buffer;

        // The single txnid optimization speeds up the case when only one
        // transaction is using the locktree. But it has the potential to
        // hurt the case when more than one txnid exists.
        //
        // There are two things we need to do to make the optimization only
        // optimize the case we care about, and not hurt the general case.
        //
        // Bound the worst-case latency for lock migration when the
        // optimization stops working:
        // - Idea: Stop the optimization and migrate immediate if we notice
        // the single txnid has takes many locks in the range buffer.
        // - Implementation: Enforce a max size on the single txnid range buffer.
        // - Analysis: Choosing the perfect max value, M, is difficult to do
        // without some feedback from the field. Intuition tells us that M should
        // not be so small that the optimization is worthless, and it should not
        // be so big that it's unreasonable to have to wait behind a thread doing
        // the work of converting M buffer locks into rangetree locks.
        //
        // Prevent concurrent-transaction workloads from trying the optimization
        // in vain:
        // - Idea: Don't even bother trying the optimization if we think the
        // system is in a concurrent-transaction state.
        // - Implementation: Do something even simpler than detecting whether the
        // system is in a concurent-transaction state. Just keep a "score" value
        // and some threshold. If at any time the locktree is eligible for the
        // optimization, only do it if the score is at this threshold. When you
        // actually do the optimization but someone has to migrate locks in the buffer
        // (expensive), then reset the score back to zero. Each time a txn
        // releases locks, the score is incremented by 1.
        // - Analysis: If you let the threshold be "C", then at most 1 / C txns will
        // do the optimization in a concurrent-transaction system. Similarly, it
        // takes at most C txns to start using the single txnid optimzation, which
        // is good when the system transitions from multithreaded to single threaded.
        //
        // STO_BUFFER_MAX_SIZE:
        //
        // We choose the max value to be 1 million since most transactions are smaller
        // than 1 million and we can create a rangetree of 1 million elements in
        // less than a second. So we can be pretty confident that this threshold
        // enables the optimization almost always, and prevents super pathological
        // latency issues for the first lock taken by a second thread.
        //
        // STO_SCORE_THRESHOLD:
        //
        // A simple first guess at a good value for the score threshold is 100.
        // By our analysis, we'd end up doing the optimization in vain for
        // around 1% of all transactions, which seems reasonable. Further,
        // if the system goes single threaded, it ought to be pretty quick
        // for 100 transactions to go by, so we won't have to wait long before
        // we start doing the single txind optimzation again.
        static const int STO_BUFFER_MAX_SIZE = 50 * 1024;
        static const int STO_SCORE_THRESHOLD = 100;
        int m_sto_score;

        // statistics about time spent ending the STO early
        uint64_t m_sto_end_early_count;
        tokutime_t m_sto_end_early_time;

        // Effect:
        //  Provides a hook for a helgrind suppression.
        // Returns:
        //  true if m_sto_txnid is not TXNID_NONE
        bool sto_txnid_is_valid_unsafe(void) const;

        // engine status reaches into the locktree to read some stats
        friend void locktree_manager::get_status(LTM_STATUS status);
    };                                                                                
```

#2.locktree_manager

```cpp
    // The locktree manager manages a set of locktrees, one for each open dictionary.
    // Locktrees are retrieved from the manager. When they are no longer needed, they
    // are be released by the user.
    class locktree_manager {
    public:
        // param: create_cb, called just after a locktree is first created.
        //        destroy_cb, called just before a locktree is destroyed.
        //        escalate_cb, called after a locktree is escalated (with extra param)
        void create(lt_create_cb create_cb, lt_destroy_cb destroy_cb, lt_escalate_cb escalate_cb, void *extra);

        void destroy(void);

        size_t get_max_lock_memory(void);

        int set_max_lock_memory(size_t max_lock_memory);

        // effect: Get a locktree from the manager. If a locktree exists with the given
        //         dict_id, it is referenced and then returned. If one did not exist, it
        //         is created. It will use the comparator for comparing keys. The on_create
        //         callback (passed to locktree_manager::create()) will be called with the
        //         given extra parameter.
        locktree *get_lt(DICTIONARY_ID dict_id, const comparator &cmp, void *on_create_extra);

        void reference_lt(locktree *lt);

        // effect: Releases one reference on a locktree. If the reference count transitions
        //         to zero, the on_destroy callback is called before it gets destroyed.
        void release_lt(locktree *lt);

        void get_status(LTM_STATUS status);

        // effect: calls the iterate function on each pending lock request
        // note: holds the manager's mutex
        typedef int (*lock_request_iterate_callback)(DICTIONARY_ID dict_id,
                                                     TXNID txnid,
                                                     const DBT *left_key,
                                                     const DBT *right_key,
                                                     TXNID blocking_txnid,
                                                     uint64_t start_time,
                                                     void *extra);
        int iterate_pending_lock_requests(lock_request_iterate_callback cb, void *extra);
        // effect: Determines if too many locks or too much memory is being used,
        //         Runs escalation on the manager if so.
        // param: big_txn, if the current transaction is 'big' (has spilled rollback logs)
        // returns: 0 if there enough resources to create a new lock, or TOKUDB_OUT_OF_LOCKS
        //          if there are not enough resources and lock escalation failed to free up
        //          enough resources for a new lock.
        int check_current_lock_constraints(bool big_txn);

        bool over_big_threshold(void);

        void note_mem_used(uint64_t mem_used);

        void note_mem_released(uint64_t mem_freed);

        bool out_of_locks(void) const;

        // Escalate all locktrees
        void escalate_all_locktrees(void);

        // Escalate a set of locktrees
        void escalate_locktrees(locktree **locktrees, int num_locktrees);

        // effect: calls the private function run_escalation(), only ok to
        //         do for tests.
        // rationale: to get better stress test coverage, we want a way to
        //            deterministicly trigger lock escalation.
        void run_escalation_for_test(void);
        void run_escalation(void);

        // Add time t to the escalator's wait time statistics
        void add_escalator_wait_time(uint64_t t);

    private:
        static const uint64_t DEFAULT_MAX_LOCK_MEMORY = 64L * 1024 * 1024;

        // tracks the current number of locks and lock memory
        uint64_t m_max_lock_memory;
        uint64_t m_current_lock_memory;
        struct lt_counters m_lt_counters;

        // the create and destroy callbacks for the locktrees
        lt_create_cb m_lt_create_callback;
        lt_destroy_cb m_lt_destroy_callback;
        lt_escalate_cb m_lt_escalate_callback;
        void *m_lt_escalate_callback_extra;

        omt<locktree *> m_locktree_map;

        // the manager's mutex protects the locktree map
        toku_mutex_t m_mutex;

        void mutex_lock(void);

        void mutex_unlock(void);

        // Manage the set of open locktrees
        locktree *locktree_map_find(const DICTIONARY_ID &dict_id);
        void locktree_map_put(locktree *lt);
        void locktree_map_remove(locktree *lt);

        static int find_by_dict_id(locktree *const &lt, const DICTIONARY_ID &dict_id);

        void escalator_init(void);
        void escalator_destroy(void);

        // statistics about lock escalation.
        toku_mutex_t m_escalation_mutex;
        uint64_t m_escalation_count;
        tokutime_t m_escalation_time;
        uint64_t m_escalation_latest_result;
        uint64_t m_wait_escalation_count;
        uint64_t m_wait_escalation_time;
        uint64_t m_long_wait_escalation_count;
        uint64_t m_long_wait_escalation_time;
        locktree_escalator m_escalator;

        friend class manager_unit_test;
    };        
```

#3.concurrent_tree

```cpp
// A concurrent_tree stores non-overlapping ranges.
// Access to disjoint parts of the tree usually occurs concurrently.

class concurrent_tree {
public:

    // effect: initialize the tree to an empty state
    void create(const comparator *cmp);

    // effect: destroy the tree.
    // requires: tree is empty
    void destroy(void);

    // returns: true iff the tree is empty
    bool is_empty(void);

    // returns: the memory overhead of a single insertion into the tree
    static uint64_t get_insertion_memory_overhead(void);

private:
    // the root needs to always exist so there's a lock to grab
    // even if the tree is empty. that's why we store a treenode
    // here and not a pointer to one.
    treenode m_root;

    friend class concurrent_tree_unit_test;
};       
```

#4. locked_keyrange

```cpp
    // A locked_keyrange gives you exclusive access to read and write
    // operations that occur on any keys in that range. You only have
    // the right to operate on keys in that range or keys that were read
    // from the keyrange using iterate()
    //
    // Access model:
    // - user prepares a locked keyrange. all threads serialize behind prepare().
    // - user breaks the serialzation point by acquiring a range, or releasing.
    // - one thread operates on a certain locked_keyrange object at a time.
    // - when the thread is finished, it releases

    class locked_keyrange {
    public:
        // effect: prepare to acquire a locked keyrange over the given
        //         concurrent_tree, preventing other threads from preparing
        //         until this thread either does acquire() or release().
        // note: operations performed on a prepared keyrange are equivalent
        //         to ones performed on an acquired keyrange over -inf, +inf.
        // rationale: this provides the user with a serialization point for descending
        //            or modifying the the tree. it also proives a convenient way of
        //            doing serializable operations on the tree.
        // There are two valid sequences of calls:
        //  - prepare, acquire, [operations], release
        //  - prepare, [operations],release
        void prepare(concurrent_tree *tree);

        // requires: the locked keyrange was prepare()'d
        // effect: acquire a locked keyrange over the given concurrent_tree.
        //         the locked keyrange represents the range of keys overlapped
        //         by the given range
        void acquire(const keyrange &range);

        // effect: releases a locked keyrange and the mutex it holds
        void release(void);

        // effect: iterate over each range this locked_keyrange represents,
        //         calling function->fn() on each node's keyrange and txnid
        //         until there are no more or the function returns false
        template <class F>
        void iterate(F *function) const;
        // inserts the given range into the tree, with an associated txnid.
        // requires: range does not overlap with anything in this locked_keyrange
        // rationale: caller is responsible for only inserting unique ranges
        void insert(const keyrange &range, TXNID txnid);

        // effect: removes the given range from the tree
        // requires: range exists exactly in this locked_keyrange
        // rationale: caller is responsible for only removing existing ranges
        void remove(const keyrange &range);

        // effect: removes all of the keys represented by this locked keyrange
        // rationale: we'd like a fast way to empty out a tree
        void remove_all(void);

    private:
        // the concurrent tree this locked keyrange is for
        concurrent_tree *m_tree;

        // the range of keys this locked keyrange represents
        keyrange m_range;

        // the subtree under which all overlapping ranges exist
        treenode *m_subtree;

        friend class concurrent_tree_unit_test;
    };        
```

#5. keyrange

```cpp
// A keyrange has a left and right key as endpoints.
//
// When a keyrange is created it owns no memory, but when it copies
// or extends another keyrange, it copies memory as necessary. This
// means it is cheap in the common case.

class keyrange {
public:

    // effect: constructor that borrows left and right key pointers.
    //         no memory is allocated or copied.
    void create(const DBT *left_key, const DBT *right_key);

    // effect: constructor that allocates and copies another keyrange's points.
    void create_copy(const keyrange &range);

    // effect: destroys the keyrange, freeing any allocated memory
    void destroy(void);

    // effect: extends the keyrange by choosing the leftmost and rightmost
    //         endpoints from this range and the given range.
    //         replaced keys in this range are freed, new keys are copied.
    void extend(const comparator &cmp, const keyrange &range);

    // returns: the amount of memory this keyrange takes. does not account
    //          for point optimizations or malloc overhead.
    uint64_t get_memory_size(void) const;

    // returns: pointer to the left key of this range
    const DBT *get_left_key(void) const;

    // returns: pointer to the right key of this range
    const DBT *get_right_key(void) const;

    // two ranges are either equal, lt, gt, or overlapping
    enum comparison {
        EQUALS,
        LESS_THAN,
        GREATER_THAN,
        OVERLAPS
    };
    // effect: compares this range to the given range
    // returns: LESS_THAN    if given range is strictly to the left
    //          GREATER_THAN if given range is strictly to the right
    //          EQUALS       if given range has the same left and right endpoints
    //          OVERLAPS     if at least one of the given range's endpoints falls
    //                       between this range's endpoints
    comparison compare(const comparator &cmp, const keyrange &range) const;

    // returns: true if the range and the given range are equal or overlapping
    bool overlaps(const comparator &cmp, const keyrange &range) const;

    // returns: a keyrange representing -inf, +inf
    static keyrange get_infinite_range(void);

private:
    // some keys should be copied, some keys should not be.
    //
    // to support both, we use two DBTs for copies and two pointers
    // for temporaries. the access rule is:
    // - if a pointer is non-null, then it reprsents the key.
    // - otherwise the pointer is null, and the key is in the copy.
    DBT m_left_key_copy;
    DBT m_right_key_copy;
    const DBT *m_left_key;
    const DBT *m_right_key;

    // if this range is a point range, then m_left_key == m_right_key
    // and the actual data is stored exactly once in m_left_key_copy.
    bool m_point_range;

    // effect: initializes a keyrange to be empty
    void init_empty(void);

    // effect: copies the given key once into the left key copy
    //         and sets the right key copy to share the left.
    // rationale: optimization for point ranges to only do one malloc
    void set_both_keys(const DBT *key);

    // effect: destroys the current left key. sets and copies the new one.
    void replace_left_key(const DBT *key);

    // effect: destroys the current right key. sets and copies the new one.
    void replace_right_key(const DBT *key);
};
```